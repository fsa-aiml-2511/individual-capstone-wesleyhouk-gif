{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 03: Classification Model\n\n**Student Name:** [Your Name]\n\n**Dataset:** [Your Dataset Name]\n\n**Original Target Variable:** [Your Original Numerical Target]\n\n**Binned Target Variable:** [Your Categorical Target - e.g., Low/Medium/High]\n\n**Checkpoint 4 Due:** Feb 22\n\n---\n\n## Rules & Integrity\n\n1. **NO AI TOOLS**: You may **NOT** use ChatGPT, Claude, Gemini, GitHub Copilot, or any other AI assistant to generate code for this assignment. The goal is to build *your* fundamental skills. If you rely on AI now, the advanced topics later will be impossible.\n\n2. **Study Groups Encouraged**: You **ARE** encouraged to discuss ideas, share approaches, and explain concepts to your study group peers. Teaching others is the best way to learn! However, the code you submit must be **your own work**.\n\n3. **Use Your Resources**: You are free to use Google, StackOverflow, Pandas/Scikit-learn documentation, and your class notes.\n\n4. **Comment Your Code**: Include comments explaining *why* you're doing what you're doing. I want to see your thought process.\n\n5. **Resubmission**: You may submit this assignment multiple times for feedback before the checkpoint deadline.\n\n---\n\n## Important: Written Reflections\n\nThroughout this notebook, you'll be asked to interpret results, justify decisions, and explain your reasoning. **These written reflections are a critical part of your grade.**\n\nYour binning justification, model selection reasoning, and analysis of the confusion matrix demonstrate your understanding. These reflections are what employers look forâ€”the ability to communicate technical decisions clearly.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn - preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Sklearn - models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sklearn - evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Model saving\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your original target variable\n",
    "ORIGINAL_TARGET = 'your_target_column'  # <-- UPDATE THIS\n",
    "\n",
    "print(f\"Original Target: {ORIGINAL_TARGET}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(df[ORIGINAL_TARGET].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Create Binned Target\n",
    "\n",
    "Convert your regression target into classification categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Binning Strategy Justification\n",
    "\n",
    "**Your approved binning strategy:**\n",
    "\n",
    "- **Number of categories:** [e.g., 3]\n",
    "- **Category names:** [e.g., Low, Medium, High]\n",
    "- **Thresholds:** [e.g., Low: < $50,000, Medium: $50,000-$100,000, High: > $100,000]\n",
    "\n",
    "**Why this makes sense for your problem:**\n",
    "\n",
    "[Explain your reasoning - this should match what you submitted to Abishek on Slack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the distribution before binning\n",
    "print(\"Target distribution:\")\n",
    "print(f\"Min: {df[ORIGINAL_TARGET].min():.2f}\")\n",
    "print(f\"25th percentile: {df[ORIGINAL_TARGET].quantile(0.25):.2f}\")\n",
    "print(f\"Median: {df[ORIGINAL_TARGET].median():.2f}\")\n",
    "print(f\"75th percentile: {df[ORIGINAL_TARGET].quantile(0.75):.2f}\")\n",
    "print(f\"Max: {df[ORIGINAL_TARGET].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your binned target using YOUR approved binning strategy\n",
    "#\n",
    "# Steps:\n",
    "# 1. Define your bin edges (e.g., bins = [min-1, threshold1, threshold2, max+1])\n",
    "# 2. Define your category labels (e.g., labels = ['Low', 'Medium', 'High'])\n",
    "# 3. Use pd.cut() to create the binned column\n",
    "#\n",
    "# Example:\n",
    "# bins = [df[ORIGINAL_TARGET].min()-1, 50000, 100000, df[ORIGINAL_TARGET].max()+1]\n",
    "# labels = ['Low', 'Medium', 'High']\n",
    "# df['target_category'] = pd.cut(df[ORIGINAL_TARGET], bins=bins, labels=labels)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "bins = [...]  # Define your bin edges\n",
    "labels = [...]  # Define your category names\n",
    "\n",
    "df['target_category'] = pd.cut(df[ORIGINAL_TARGET], bins=bins, labels=labels)\n",
    "\n",
    "print(\"Binned target distribution:\")\n",
    "print(df['target_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the binned distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original distribution with bin lines\n",
    "axes[0].hist(df[ORIGINAL_TARGET], bins=30, edgecolor='black', alpha=0.7)\n",
    "for edge in bins[1:-1]:  # Skip first and last (min/max)\n",
    "    axes[0].axvline(edge, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel(ORIGINAL_TARGET)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Original Distribution with Bin Boundaries')\n",
    "\n",
    "# Binned category counts\n",
    "df['target_category'].value_counts().plot(kind='bar', ax=axes[1], edgecolor='black')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Binned Target Distribution')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check Class Balance\n",
    "\n",
    "*Is your binned target reasonably balanced?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "class_counts = df['target_category'].value_counts()\n",
    "class_percentages = df['target_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for cat in class_counts.index:\n",
    "    print(f\"{cat}: {class_counts[cat]} ({class_percentages[cat]:.1f}%)\")\n",
    "\n",
    "# Check for severe imbalance\n",
    "min_class_pct = class_percentages.min()\n",
    "if min_class_pct < 10:\n",
    "    print(f\"\\nWarning: Smallest class is only {min_class_pct:.1f}% of data.\")\n",
    "    print(\"Consider adjusting your binning strategy.\")\n",
    "else:\n",
    "    print(f\"\\nClass balance looks reasonable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Consistency Check\n",
    "\n",
    "**Important:** For consistency, you should use the same features for classification as you did for regression. This makes your app simpler (users enter the same inputs for both models) and allows for fair comparison.\n",
    "\n",
    "If you decide to use different features, provide a strong justification below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features you selected in the regression notebook\n",
    "try:\n",
    "    regression_features = joblib.load('../models/regression_features.pkl')\n",
    "    print(\"Features from Regression Model:\")\n",
    "    print(regression_features)\n",
    "    print(f\"\\nNumber of regression features: {len(regression_features)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: regression_features.pkl not found!\")\n",
    "    print(\"Make sure you've completed Notebook 02 and saved your regression model first.\")\n",
    "    regression_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Decision\n",
    "\n",
    "**Will you use the same features as your regression model?**\n",
    "\n",
    "- [ ] **Yes, same features** (Recommended for simpler deployment)\n",
    "- [ ] **No, different features** (Requires justification below)\n",
    "\n",
    "**If using different features, justify why:**\n",
    "\n",
    "[Write your justification here - e.g., \"For classification, feature X is less predictive of categories than it was for the continuous target...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECIDE: Use regression features or define new ones?\n",
    "\n",
    "# Option A: Use the same features as regression (RECOMMENDED)\n",
    "if regression_features is not None:\n",
    "    SELECTED_FEATURES = regression_features\n",
    "    print(\"Using same features as regression model.\")\n",
    "else:\n",
    "    # Option B: Define your own features (if regression not done yet)\n",
    "    SELECTED_FEATURES = [\n",
    "        # 'feature1',\n",
    "        # 'feature2',\n",
    "        # 'feature3',\n",
    "        # etc.\n",
    "    ]\n",
    "    print(\"Defining features manually.\")\n",
    "\n",
    "print(f\"\\nSelected features for classification ({len(SELECTED_FEATURES)}):\")\n",
    "for i, f in enumerate(SELECTED_FEATURES, 1):\n",
    "    print(f\"  {i}. {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix using SELECTED features\n",
    "X = df[SELECTED_FEATURES].copy()\n",
    "y = df['target_category']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {X.columns.tolist()}\")\n",
    "print(f\"\\nTarget classes: {y.unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Label Encoding\n",
    "\n",
    "Many sklearn models require numeric targets. Use LabelEncoder to convert category names to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode your target labels to numbers using LabelEncoder\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a LabelEncoder instance\n",
    "# 2. Fit and transform y to create y_encoded\n",
    "#\n",
    "# Hint: label_encoder.fit_transform(y)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify encoding (run this to check)\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Train-Test Split\n",
    "\n",
    "For classification, we use **stratified** splitting to ensure each class is proportionally represented in both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split your data with stratification\n",
    "#\n",
    "# Requirements:\n",
    "# - 80/20 split (test_size=0.2)\n",
    "# - random_state=42 for reproducibility\n",
    "# - stratify=y_encoded (THIS IS THE KEY DIFFERENCE FROM REGRESSION!)\n",
    "#   This ensures each class is proportionally represented in train/test\n",
    "#\n",
    "# Store in: X_train, X_test, y_train, y_test\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split and stratification\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  {label_encoder.classes_[u]}: {c} ({c/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale your features using StandardScaler\n",
    "#\n",
    "# Same as regression:\n",
    "# 1. Create StandardScaler instance\n",
    "# 2. fit_transform on X_train\n",
    "# 3. transform (only!) on X_test\n",
    "#\n",
    "# Store in: X_train_scaled, X_test_scaled\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame (helpful for later)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"Scaler fitted on {len(SELECTED_FEATURES)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function evaluates any classifier - you'll use it throughout\n",
    "def evaluate_classifier(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train classifier and return evaluation metrics.\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Test Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'Precision (weighted)': precision_score(y_test, y_test_pred, average='weighted'),\n",
    "        'Recall (weighted)': recall_score(y_test, y_test_pred, average='weighted'),\n",
    "        'F1 (weighted)': f1_score(y_test, y_test_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    return results, model, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a baseline Logistic Regression model\n",
    "#\n",
    "# Create LogisticRegression(random_state=42, max_iter=1000)\n",
    "# Use evaluate_classifier() to train and evaluate\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# baseline_model = ...\n",
    "# baseline_results, baseline_trained, baseline_preds = evaluate_classifier(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE MODEL: Logistic Regression\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train Accuracy: {baseline_results['Train Accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy:  {baseline_results['Test Accuracy']:.4f}\")\n",
    "print(f\"F1 Score:       {baseline_results['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and display the confusion matrix for baseline\n",
    "#\n",
    "# Steps:\n",
    "# 1. Use confusion_matrix(y_test, baseline_preds) to create the matrix\n",
    "# 2. Use ConfusionMatrixDisplay to visualize it\n",
    "#\n",
    "# Hint: ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Confusion Matrix\n",
    "\n",
    "The confusion matrix shows where your model gets predictions right and wrong:\n",
    "\n",
    "- **Diagonal cells (top-left to bottom-right):** Correct predictions\n",
    "- **Off-diagonal cells:** Misclassifications\n",
    "- **Row:** What the actual class was\n",
    "- **Column:** What the model predicted\n",
    "\n",
    "**How to read it:** \n",
    "- If a row has many values spread across columns, that class is often confused with others\n",
    "- If a column has many values from different rows, the model is predicting that class too often\n",
    "\n",
    "**Example interpretation:**\n",
    "- \"20 Low samples were correctly predicted as Low\"\n",
    "- \"5 Medium samples were incorrectly predicted as High\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Interpretation\n",
    "\n",
    "*Analyze your baseline results:*\n",
    "\n",
    "**Questions to answer:**\n",
    "- How well does the baseline perform?\n",
    "- Which classes are easiest/hardest to predict (look at the diagonal)?\n",
    "- Where does the model get confused (look at off-diagonal cells)?\n",
    "- Is there overfitting (train vs test accuracy gap)?\n",
    "\n",
    "**Your interpretation:**\n",
    "\n",
    "[Write your interpretation here - be specific about which categories the model struggles with]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Cross-Validation\n",
    "\n",
    "Cross-validation gives us a more robust estimate of model performance by training on different portions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for key models\n",
    "print(\"5-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "models_to_cv = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "for name, model in models_to_cv.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean Accuracy': scores.mean(),\n",
    "        'CV Std': scores.std()\n",
    "    })\n",
    "    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Interpretation\n",
    "\n",
    "*What does the cross-validation tell you?*\n",
    "\n",
    "- Which model is most consistent (lowest std)?\n",
    "- Does the ranking change from the single train/test split?\n",
    "\n",
    "**Your interpretation:**\n",
    "\n",
    "[Write your interpretation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Model Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = [baseline_results]\n",
    "\n",
    "# Dictionary to store trained models\n",
    "trained_models = {\n",
    "    'Logistic Regression (Baseline)': baseline_trained\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Decision Tree classifier\n",
    "#\n",
    "# Create DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "# Use evaluate_classifier() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Decision Tree - Test Accuracy: {dt_results['Test Accuracy']:.4f}, F1: {dt_results['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Random Forest classifier\n",
    "#\n",
    "# Create RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "# Use evaluate_classifier()\n",
    "# Add to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest - Test Accuracy: {rf_results['Test Accuracy']:.4f}, F1: {rf_results['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a KNN classifier\n",
    "#\n",
    "# Create KNeighborsClassifier(n_neighbors=5)\n",
    "# Use evaluate_classifier()\n",
    "# Add to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"KNN - Test Accuracy: {knn_results['Test Accuracy']:.4f}, F1: {knn_results['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Gradient Boosting classifier\n",
    "#\n",
    "# Create GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "# Use evaluate_classifier()\n",
    "# Add to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Gradient Boosting - Test Accuracy: {gb_results['Test Accuracy']:.4f}, F1: {gb_results['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Your Own Model (Optional)\n",
    "\n",
    "Try a different model or different hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ADDITIONAL MODEL HERE:\n",
    "# Try SVC, different hyperparameters, or another classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = results_df['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(x - width/2, results_df['Train Accuracy'], width, label='Train Accuracy')\n",
    "axes[0].bar(x + width/2, results_df['Test Accuracy'], width, label='Test Accuracy')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "\n",
    "# F1 Score comparison\n",
    "axes[1].bar(x, results_df['F1 (weighted)'], color='green', alpha=0.7)\n",
    "axes[1].set_ylabel('F1 Score (weighted)')\n",
    "axes[1].set_title('F1 Score Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model (by test accuracy)\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {results_df.iloc[0]['F1 (weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Justification\n",
    "\n",
    "*Explain why you chose this model:*\n",
    "\n",
    "**Questions to consider:**\n",
    "- Why this model over others?\n",
    "- Is there significant overfitting?\n",
    "- How does it compare to baseline?\n",
    "- Which metrics did you prioritize and why?\n",
    "\n",
    "**Your justification:**\n",
    "\n",
    "[Write your justification here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from best model\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Classification Report\n",
    "\n",
    "The classification report shows per-class metrics:\n",
    "\n",
    "- **Precision:** Of all predictions for this class, what % were correct?\n",
    "  - High precision = Few false positives\n",
    "  - Example: \"When the model predicts 'High', it's correct 85% of the time\"\n",
    "\n",
    "- **Recall:** Of all actual instances of this class, what % did we find?\n",
    "  - High recall = Few false negatives\n",
    "  - Example: \"We correctly identified 90% of all actual 'High' cases\"\n",
    "\n",
    "- **F1-Score:** Harmonic mean of precision and recall (balanced measure)\n",
    "  - Good when you need both precision and recall\n",
    "\n",
    "- **Support:** Number of actual occurrences of each class in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(f'Best Model ({best_model_name}) Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Confusion Matrix Analysis\n",
    "\n",
    "*Analyze the confusion matrix carefully:*\n",
    "\n",
    "**For each class, describe:**\n",
    "1. How many correct predictions (diagonal)?\n",
    "2. What are the most common misclassifications?\n",
    "3. Does the confusion make sense for your problem?\n",
    "\n",
    "**Your analysis:**\n",
    "\n",
    "[Write your detailed analysis here - e.g., \"The model most often confuses Medium with High (X cases), which makes sense because...\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance['Feature'], importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance for Classification')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 5 most important features:\")\n",
    "    for i, row in importance.tail(5).iloc[::-1].iterrows():\n",
    "        print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "        \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For multi-class, coef_ has shape (n_classes, n_features)\n",
    "    coef = best_model.coef_\n",
    "    if len(coef.shape) > 1:\n",
    "        # Average absolute importance across classes\n",
    "        importance_vals = np.abs(coef).mean(axis=0)\n",
    "    else:\n",
    "        importance_vals = np.abs(coef)\n",
    "    \n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_vals\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance['Feature'], importance['Importance'])\n",
    "    plt.xlabel('Average Absolute Coefficient')\n",
    "    plt.title('Feature Importance for Classification')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Model Iteration Reflection\n",
    "\n",
    "*Reflect on your modeling process:*\n",
    "\n",
    "**1. How did your models evolve from baseline to best?**\n",
    "\n",
    "[Describe the progression and what you learned]\n",
    "\n",
    "**2. What surprised you about the results?**\n",
    "\n",
    "[Any unexpected findings?]\n",
    "\n",
    "**3. If you had more time, what would you try next?**\n",
    "\n",
    "[Future improvements to explore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 14: Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best classification model\n",
    "model_path = '../models/classification_model.pkl'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the scaler (fitted on SELECTED features)\n",
    "scaler_path = '../models/classification_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save the label encoder\n",
    "encoder_path = '../models/label_encoder.pkl'\n",
    "joblib.dump(label_encoder, encoder_path)\n",
    "print(f\"Label encoder saved to {encoder_path}\")\n",
    "\n",
    "# Save feature names (SELECTED_FEATURES)\n",
    "features_path = '../models/classification_features.pkl'\n",
    "joblib.dump(SELECTED_FEATURES, features_path)\n",
    "print(f\"Features saved to {features_path}\")\n",
    "\n",
    "# Save binning info (for reference in Streamlit app)\n",
    "binning_info = {\n",
    "    'bins': bins,\n",
    "    'labels': labels,\n",
    "    'original_target': ORIGINAL_TARGET\n",
    "}\n",
    "binning_path = '../models/binning_info.pkl'\n",
    "joblib.dump(binning_info, binning_path)\n",
    "print(f\"Binning info saved to {binning_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved model works\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "loaded_encoder = joblib.load(encoder_path)\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test.iloc[[0]]\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "test_pred = loaded_model.predict(test_sample_scaled)\n",
    "test_pred_label = loaded_encoder.inverse_transform(test_pred)\n",
    "\n",
    "print(f\"\\nModel verification:\")\n",
    "print(f\"Sample input: {test_sample.values[0][:3]}...\")\n",
    "print(f\"Predicted class: {test_pred_label[0]}\")\n",
    "print(f\"Actual class: {loaded_encoder.inverse_transform([y_test.iloc[0]])[0]}\")\n",
    "print(\"\\nModel saved and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Accomplished\n",
    "- [ ] Created binned target variable with justification\n",
    "- [ ] Verified class balance\n",
    "- [ ] Used consistent features (same as regression or justified different)\n",
    "- [ ] Split data with stratification\n",
    "- [ ] Performed cross-validation\n",
    "- [ ] Built baseline classifier\n",
    "- [ ] Tried multiple classifier types\n",
    "- [ ] Compared and selected best model\n",
    "- [ ] Analyzed model with confusion matrix and classification report\n",
    "- [ ] Saved model, scaler, encoder, and binning info\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Best Model:** [Model name]\n",
    "\n",
    "**Test Accuracy:** [Value]\n",
    "\n",
    "**F1 Score:** [Value]\n",
    "\n",
    "**Improvement over baseline:** [Percentage or description]\n",
    "\n",
    "### Comparison to Regression\n",
    "\n",
    "*How does classification performance relate to your regression results?*\n",
    "\n",
    "[Write your comparison here - consider: Are the same features important? Does predicting categories work better/worse than predicting exact values for your problem?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Move on to building your **Streamlit app** to deploy both models!\n",
    "\n",
    "You now have saved:\n",
    "- `regression_model.pkl` and `regression_scaler.pkl`\n",
    "- `classification_model.pkl`, `classification_scaler.pkl`, and `label_encoder.pkl`\n",
    "\n",
    "The Streamlit starter code in `app/app.py` will load these files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 4 Submission Instructions\n",
    "\n",
    "You've completed the classification model. Time to submit!\n",
    "\n",
    "### Step 1: Save and Close\n",
    "Make sure this notebook is saved and all cells have been run.\n",
    "\n",
    "### Step 2: Commit Your Work\n",
    "```bash\n",
    "git add .\n",
    "git commit -m \"Complete classification model - Checkpoint 4\"\n",
    "git push\n",
    "```\n",
    "\n",
    "### Step 3: Submit to Canvas\n",
    "1. Go to Canvas\n",
    "2. Find the **Checkpoint 4: Classification Model** assignment\n",
    "3. Submit the link to your GitHub repository\n",
    "\n",
    "### What Gets Graded\n",
    "- Binning justification and implementation\n",
    "- Feature consistency (same as regression or justified different)\n",
    "- Model comparison and selection reasoning\n",
    "- Confusion matrix interpretation\n",
    "- Classification report analysis\n",
    "- All written reflections completed\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}