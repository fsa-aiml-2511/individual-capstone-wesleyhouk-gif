{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 02: Regression Model\n\n**Student Name:** [Your Name]\n\n**Dataset:** [Your Dataset Name]\n\n**Target Variable:** [Your Target Column]\n\n**Checkpoint 3 Due:** Feb 15\n\n---\n\n## Rules & Integrity\n\n1. **NO AI TOOLS**: You may **NOT** use ChatGPT, Claude, Gemini, GitHub Copilot, or any other AI assistant to generate code for this assignment. The goal is to build *your* fundamental skills. If you rely on AI now, the advanced topics later will be impossible.\n\n2. **Study Groups Encouraged**: You **ARE** encouraged to discuss ideas, share approaches, and explain concepts to your study group peers. Teaching others is the best way to learn! However, the code you submit must be **your own work**.\n\n3. **Use Your Resources**: You are free to use Google, StackOverflow, Pandas/Scikit-learn documentation, and your class notes.\n\n4. **Comment Your Code**: Include comments explaining *why* you're doing what you're doing. I want to see your thought process.\n\n5. **Resubmission**: You may submit this assignment multiple times for feedback before the checkpoint deadline.\n\n---\n\n## Important: Written Reflections\n\nThroughout this notebook, you'll be asked to interpret results, justify decisions, and explain your reasoning. **These written reflections are a critical part of your grade.**\n\nGood data scientists don't just run code—they communicate their findings clearly. Take time to write thoughtful, complete responses to all reflection prompts. This demonstrates your understanding and prepares you for real-world stakeholder communication.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn - preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sklearn - models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Sklearn - evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Model saving\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data from previous notebook\n",
    "df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your target variable\n",
    "TARGET = 'your_target_column'  # <-- UPDATE THIS!\n",
    "\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(df[TARGET].describe())\n",
    "\n",
    "# Store target range for later interpretation\n",
    "target_range = df[TARGET].max() - df[TARGET].min()\n",
    "target_std = df[TARGET].std()\n",
    "print(f\"\\nTarget range: {target_range:,.2f}\")\n",
    "print(f\"Target std: {target_std:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nAll features ({len(X.columns)}):\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any non-numeric columns that need to be handled\n",
    "non_numeric = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"Warning: Non-numeric columns found: {non_numeric}\")\n",
    "    print(\"You need to encode these or go back to Notebook 01!\")\n",
    "else:\n",
    "    print(\"All features are numeric. Ready to proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Train-Test Split\n",
    "\n",
    "Split your data into training and test sets. The training set is used to train the model, and the test set is used to evaluate how well it generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split your data into training and test sets\n",
    "# \n",
    "# Requirements:\n",
    "# - Use an 80/20 split (test_size=0.2)\n",
    "# - Set random_state=42 for reproducibility\n",
    "# - Store results in: X_train, X_test, y_train, y_test\n",
    "#\n",
    "# Hint: Use train_test_split(X, y, ...)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your split (run this cell to check)\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Feature Scaling\n",
    "\n",
    "Many ML algorithms perform better when features are on similar scales. StandardScaler transforms features to have mean=0 and std=1.\n",
    "\n",
    "**Important:** Fit the scaler on training data only, then transform both train and test. This prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale your features using StandardScaler\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a StandardScaler instance\n",
    "# 2. Fit the scaler on X_train and transform X_train (use fit_transform)\n",
    "# 3. Transform X_test using the same scaler (use transform only - NOT fit_transform!)\n",
    "# 4. Store results in: X_train_scaled, X_test_scaled\n",
    "#\n",
    "# Why fit only on train? To prevent \"data leakage\" - test data should be truly unseen.\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame for easier viewing (optional but helpful)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Verify scaling worked\n",
    "print(\"Features scaled successfully!\")\n",
    "print(f\"\\nScaled feature means (should be ~0): {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"Scaled feature stds (should be ~1): {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Baseline Model\n",
    "\n",
    "Start with a simple Linear Regression to establish a baseline performance. This gives us a reference point for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function evaluates any model - you'll use it throughout this notebook\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train model and return evaluation metrics.\"\"\"\n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train R2': r2_score(y_train, y_train_pred),\n",
    "        'Test R2': r2_score(y_test, y_test_pred),\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return results, model, y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a baseline Linear Regression model\n",
    "#\n",
    "# Steps:\n",
    "# 1. Create a LinearRegression() model instance\n",
    "# 2. Use the evaluate_model() function to train and evaluate it\n",
    "# 3. Store the results\n",
    "#\n",
    "# The evaluate_model function returns: (results_dict, trained_model, predictions)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# baseline_model = ...\n",
    "# baseline_results, baseline_trained, baseline_preds = evaluate_model(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE MODEL: Linear Regression\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train R²: {baseline_results['Train R2']:.4f}\")\n",
    "print(f\"Test R²:  {baseline_results['Test R2']:.4f}\")\n",
    "print(f\"Test RMSE: {baseline_results['Test RMSE']:,.2f}\")\n",
    "print(f\"Test MAE: {baseline_results['Test MAE']:,.2f}\")\n",
    "\n",
    "# Context for RMSE\n",
    "print(f\"\\n--- RMSE in Context ---\")\n",
    "print(f\"RMSE as % of target range: {baseline_results['Test RMSE']/target_range*100:.1f}%\")\n",
    "print(f\"RMSE as % of target std: {baseline_results['Test RMSE']/target_std*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Interpretation\n",
    "\n",
    "*Analyze your baseline results:*\n",
    "\n",
    "**Understanding your metrics:**\n",
    "- **R² (0 to 1)**: How much variance in target is explained by features. 0.7+ is often considered good.\n",
    "- **RMSE**: Average prediction error in the same units as your target. Lower is better.\n",
    "- **RMSE as % of range**: Helps you understand if errors are big or small relative to your data.\n",
    "\n",
    "**Questions to answer:**\n",
    "- What does your R² score tell you about how well features explain the target?\n",
    "- Is there a big gap between train and test R²? (Gap > 0.1 could indicate overfitting)\n",
    "- Is your RMSE reasonable? (e.g., if predicting house prices, is a $20K error acceptable?)\n",
    "\n",
    "**Your interpretation:**\n",
    "\n",
    "[Write your interpretation here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Model Iteration\n",
    "\n",
    "Try at least 2-3 different models to see if you can improve on the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results for comparison\n",
    "all_results = [baseline_results]\n",
    "\n",
    "# Dictionary to store trained models\n",
    "trained_models = {\n",
    "    'Linear Regression (Baseline)': baseline_trained\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Ridge Regression\n",
    "\n",
    "Ridge adds L2 regularization to prevent overfitting by penalizing large coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Ridge Regression model\n",
    "#\n",
    "# Create a Ridge model with alpha=1.0\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results list\n",
    "# Add trained model to trained_models dict\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Ridge Regression - Test R²: {ridge_results['Test R2']:.4f}, Test RMSE: {ridge_results['Test RMSE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Lasso Regression\n",
    "\n",
    "Lasso adds L1 regularization, which can zero out unimportant features (automatic feature selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Lasso Regression model\n",
    "#\n",
    "# Create a Lasso model with alpha=0.1\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Lasso Regression - Test R²: {lasso_results['Test R2']:.4f}, Test RMSE: {lasso_results['Test RMSE']:,.2f}\")\n",
    "\n",
    "# Show which features Lasso kept (non-zero coefficients)\n",
    "lasso_coefs = pd.Series(lasso_trained.coef_, index=X.columns)\n",
    "kept_features = lasso_coefs[lasso_coefs != 0]\n",
    "print(f\"\\nLasso kept {len(kept_features)} of {len(X.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Decision Tree model\n",
    "#\n",
    "# Create a DecisionTreeRegressor with max_depth=10 and random_state=42\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Decision Tree - Test R²: {dt_results['Test R2']:.4f}, Test RMSE: {dt_results['Test RMSE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and evaluate a Random Forest model\n",
    "#\n",
    "# Create a RandomForestRegressor with:\n",
    "#   - n_estimators=100\n",
    "#   - max_depth=10\n",
    "#   - random_state=42\n",
    "#   - n_jobs=-1 (use all CPU cores)\n",
    "#\n",
    "# Use evaluate_model() to train and evaluate\n",
    "# Add results to all_results and trained_models\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest - Test R²: {rf_results['Test R2']:.4f}, Test RMSE: {rf_results['Test RMSE']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Your Own Model (Optional)\n",
    "\n",
    "*Feel free to try additional models or tune hyperparameters!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ADDITIONAL MODEL HERE:\n",
    "# Try GradientBoostingRegressor, different hyperparameters, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Cross-Validation (More Robust Evaluation)\n",
    "\n",
    "Cross-validation gives us a more reliable estimate of model performance by testing on multiple different train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on top models\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cv_results = []\n",
    "\n",
    "models_to_cv = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "for name, model in models_to_cv.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'CV Mean R²': scores.mean(),\n",
    "        'CV Std R²': scores.std()\n",
    "    })\n",
    "    print(f\"{name}: R² = {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Cross-Validation Matters:**\n",
    "- A model that performs well on one train/test split might just be lucky\n",
    "- CV tests on 5 different splits, giving us confidence in the results\n",
    "- Lower standard deviation = more consistent/reliable model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.round(4)\n",
    "results_df = results_df.sort_values('Test R2', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# R² Comparison\n",
    "models = results_df['Model']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train R2'], width, label='Train R²', alpha=0.8)\n",
    "axes[0].bar(x + width/2, results_df['Test R2'], width, label='Test R²', alpha=0.8)\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_title('R² Comparison Across Models')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[1].bar(x - width/2, results_df['Train RMSE'], width, label='Train RMSE', alpha=0.8)\n",
    "axes[1].bar(x + width/2, results_df['Test RMSE'], width, label='Test RMSE', alpha=0.8)\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('RMSE Comparison Across Models')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Iteration Reflection\n",
    "\n",
    "*Before selecting your best model, reflect on what you learned:*\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which model improved most over the baseline?\n",
    "- Did any models show signs of overfitting (big train vs test gap)?\n",
    "- Did regularization (Ridge/Lasso) help or hurt performance?\n",
    "- Did tree-based models (Decision Tree, Random Forest) work better than linear models?\n",
    "\n",
    "**Your reflection:**\n",
    "\n",
    "[Write your reflection here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Feature Importance & Selection\n",
    "\n",
    "**Important:** Your final model should use only **4-8 features**. This section helps you identify which features matter most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (works well for this)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_trained.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(rf_importance)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(rf_importance['Feature'][::-1], rf_importance['Importance'][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check correlations with target\n",
    "correlations = X_train.corrwith(y_train).abs().sort_values(ascending=False)\n",
    "print(\"Absolute Correlations with Target:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select your top features (4-8 features)\n",
    "#\n",
    "# Based on the importance analysis above, choose your best features.\n",
    "# Consider both Random Forest importance AND correlations.\n",
    "# Also think about what makes sense from a domain perspective.\n",
    "\n",
    "SELECTED_FEATURES = [\n",
    "    # YOUR FEATURES HERE - add 4-8 feature names:\n",
    "    # 'feature1',\n",
    "    # 'feature2',\n",
    "    # etc.\n",
    "]\n",
    "\n",
    "# Fallback if you haven't selected yet\n",
    "if len(SELECTED_FEATURES) == 0:\n",
    "    SELECTED_FEATURES = rf_importance['Feature'].head(6).tolist()\n",
    "    print(f\"Using top 6 features from Random Forest: {SELECTED_FEATURES}\")\n",
    "else:\n",
    "    print(f\"Selected features ({len(SELECTED_FEATURES)}): {SELECTED_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with selected features only\n",
    "X_train_selected = X_train_scaled[SELECTED_FEATURES]\n",
    "X_test_selected = X_test_scaled[SELECTED_FEATURES]\n",
    "\n",
    "print(f\"Training with {len(SELECTED_FEATURES)} selected features...\")\n",
    "\n",
    "# Test a few models with selected features\n",
    "selected_results = []\n",
    "\n",
    "for name, model in [('Linear Regression', LinearRegression()),\n",
    "                    ('Ridge', Ridge(alpha=1.0)),\n",
    "                    ('Random Forest', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42))]:\n",
    "    results, trained, _ = evaluate_model(model, X_train_selected, X_test_selected, y_train, y_test, name)\n",
    "    selected_results.append(results)\n",
    "    print(f\"{name} with {len(SELECTED_FEATURES)} features - Test R²: {results['Test R2']:.4f}\")\n",
    "\n",
    "selected_df = pd.DataFrame(selected_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Justification\n",
    "\n",
    "**Questions to answer:**\n",
    "- Which features did you select and why?\n",
    "- Did performance drop significantly with fewer features?\n",
    "- Do these features make sense from a domain perspective?\n",
    "- These are the features users will input in your Streamlit app—are they reasonable to ask for?\n",
    "\n",
    "**Your justification:**\n",
    "\n",
    "[Write your justification here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose and train your final model with selected features\n",
    "#\n",
    "# Based on your analysis, pick the best model type and instantiate it.\n",
    "# Consider: performance, consistency, simplicity\n",
    "#\n",
    "# Example: final_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "# final_model = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate final model\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "y_pred = final_model.predict(X_test_selected)\n",
    "\n",
    "# Final metrics\n",
    "final_r2 = r2_score(y_test, y_pred)\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "final_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {type(final_model).__name__}\")\n",
    "print(f\"Features: {SELECTED_FEATURES}\")\n",
    "print(f\"\\nTest R²: {final_r2:.4f}\")\n",
    "print(f\"Test RMSE: {final_rmse:,.2f}\")\n",
    "print(f\"Test MAE: {final_mae:,.2f}\")\n",
    "print(f\"\\nRMSE as % of target range: {final_rmse/target_range*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Justification\n",
    "\n",
    "*Explain why you chose this model as your best:*\n",
    "\n",
    "**Questions to consider:**\n",
    "- Why did you select this model over others?\n",
    "- Is there significant overfitting (train vs test gap)?\n",
    "- How does the performance compare to your baseline?\n",
    "- Would a simpler model be almost as good?\n",
    "- Does the RMSE represent acceptable prediction error for your problem?\n",
    "\n",
    "**Your justification:**\n",
    "\n",
    "[Write your justification here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Actual vs Predicted ({type(final_model).__name__})')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual distribution (should be roughly normal, centered at 0)\n",
    "axes[0].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Residuals')\n",
    "\n",
    "# Residuals vs Predicted (should show no pattern)\n",
    "axes[1].scatter(y_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residuals vs Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual mean (should be ~0): {residuals.mean():.2f}\")\n",
    "print(f\"Residual std: {residuals.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for final model (with selected features)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': SELECTED_FEATURES,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(final_importance['Feature'], final_importance['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance (Final Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    final_importance = pd.DataFrame({\n",
    "        'Feature': SELECTED_FEATURES,\n",
    "        'Coefficient': final_model.coef_\n",
    "    }).sort_values('Coefficient', key=abs, ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['green' if c > 0 else 'red' for c in final_importance['Coefficient']]\n",
    "    plt.barh(final_importance['Feature'], final_importance['Coefficient'], color=colors)\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.title('Feature Coefficients (Final Model)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new scaler fitted only on selected features\n",
    "final_scaler = StandardScaler()\n",
    "X_train_final = X_train[SELECTED_FEATURES]\n",
    "final_scaler.fit(X_train_final)\n",
    "\n",
    "# Save the best model\n",
    "model_path = '../models/regression_model.pkl'\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save the scaler (fitted on selected features only)\n",
    "scaler_path = '../models/regression_scaler.pkl'\n",
    "joblib.dump(final_scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save feature names (the selected features for Streamlit app)\n",
    "features_path = '../models/regression_features.pkl'\n",
    "joblib.dump(SELECTED_FEATURES, features_path)\n",
    "print(f\"Features saved to {features_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved model works\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "loaded_features = joblib.load(features_path)\n",
    "\n",
    "# Test prediction\n",
    "test_sample = X_test[loaded_features].iloc[[0]]\n",
    "test_sample_scaled = loaded_scaler.transform(test_sample)\n",
    "test_pred = loaded_model.predict(test_sample_scaled)\n",
    "\n",
    "print(f\"\\nModel verification:\")\n",
    "print(f\"Features used: {loaded_features}\")\n",
    "print(f\"Sample input: {test_sample.values[0]}\")\n",
    "print(f\"Predicted: {test_pred[0]:,.2f}\")\n",
    "print(f\"Actual: {y_test.iloc[0]:,.2f}\")\n",
    "print(f\"\\nModel saved and verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Binning Strategy for Classification\n",
    "\n",
    "**IMPORTANT:** Before you start Notebook 03, you need to send your binning strategy to Abishek on Slack for approval by **Feb 15**.\n",
    "\n",
    "### Analyze Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at target distribution to help decide binning\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"\\nDescriptive Statistics:\")\n",
    "print(y.describe())\n",
    "\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [10, 25, 33, 50, 67, 75, 90]:\n",
    "    print(f\"{p}th percentile: {y.quantile(p/100):,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize potential binning strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram with quartile lines\n",
    "axes[0].hist(y, bins=30, edgecolor='black', alpha=0.7)\n",
    "for p, color, style in [(25, 'orange', '--'), (50, 'red', '-'), (75, 'orange', '--')]:\n",
    "    axes[0].axvline(y.quantile(p/100), color=color, linestyle=style, linewidth=2,\n",
    "                     label=f'{p}th percentile: {y.quantile(p/100):,.0f}')\n",
    "axes[0].set_xlabel(TARGET)\n",
    "axes[0].set_title('Distribution with Quartile Lines')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(y)\n",
    "axes[1].set_ylabel(TARGET)\n",
    "axes[1].set_title('Box Plot of Target')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Binning Strategy\n",
    "\n",
    "*Complete this section and send to Abishek on Slack by Feb 15:*\n",
    "\n",
    "**1. How many categories will you create?**\n",
    "\n",
    "[Your answer - e.g., 3 categories: Low, Medium, High]\n",
    "\n",
    "**2. What are your bin thresholds?**\n",
    "\n",
    "[Your answer - e.g., Low: < $30,000, Medium: $30,000-$60,000, High: > $60,000]\n",
    "\n",
    "**3. Why does this binning make sense for your problem?**\n",
    "\n",
    "[Your answer - explain the domain reasoning. Why are these meaningful categories?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview your binning\n",
    "def create_bins_preview(y, strategy='quartile'):\n",
    "    if strategy == 'quartile':\n",
    "        bins = [y.min()-1, y.quantile(0.25), y.quantile(0.75), y.max()+1]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "    elif strategy == 'tertile':\n",
    "        bins = [y.min()-1, y.quantile(0.33), y.quantile(0.67), y.max()+1]\n",
    "        labels = ['Low', 'Medium', 'High']\n",
    "    # Add your custom strategy here if needed\n",
    "    \n",
    "    return pd.cut(y, bins=bins, labels=labels)\n",
    "\n",
    "# Preview with quartile binning\n",
    "y_binned = create_bins_preview(y, 'quartile')\n",
    "print(\"Preview of binned target (using quartiles):\")\n",
    "print(y_binned.value_counts().sort_index())\n",
    "print(f\"\\nPercentages:\")\n",
    "print((y_binned.value_counts(normalize=True) * 100).round(1).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Accomplished\n",
    "- [ ] Loaded and prepared cleaned data\n",
    "- [ ] Split data into train and test sets\n",
    "- [ ] Scaled features appropriately\n",
    "- [ ] Built a baseline model\n",
    "- [ ] Tried multiple model types\n",
    "- [ ] Performed cross-validation\n",
    "- [ ] Selected top 4-8 features\n",
    "- [ ] Compared and selected best model\n",
    "- [ ] Analyzed model performance (residuals, feature importance)\n",
    "- [ ] Saved model, scaler, and feature list\n",
    "- [ ] Planned binning strategy for classification\n",
    "\n",
    "### Key Results\n",
    "\n",
    "**Best Model:** [Model name]\n",
    "\n",
    "**Selected Features:** [List them]\n",
    "\n",
    "**Test R²:** [Value]\n",
    "\n",
    "**Test RMSE:** [Value]\n",
    "\n",
    "**Improvement over baseline:** [Percentage or description]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 3 Submission Instructions\n",
    "\n",
    "**Congratulations!** You've completed Checkpoint 3 (Regression Model).\n",
    "\n",
    "### Step 1: Save This Notebook\n",
    "- File -> Save (or Ctrl+S / Cmd+S)\n",
    "\n",
    "### Step 2: Send Binning Strategy to Abishek\n",
    "- Message Abishek on Slack with your binning strategy\n",
    "- Include: number of categories, thresholds, and justification\n",
    "\n",
    "### Step 3: Commit to GitHub\n",
    "\n",
    "```bash\n",
    "# Stage your changes\n",
    "git add notebooks/02_regression_model.ipynb\n",
    "git add models/\n",
    "\n",
    "# Commit with a meaningful message\n",
    "git commit -m \"Complete Checkpoint 3: Regression model with feature selection\"\n",
    "\n",
    "# Push to GitHub\n",
    "git push\n",
    "```\n",
    "\n",
    "### Step 4: Submit to Canvas\n",
    "1. Go to the Checkpoint 3 assignment on Canvas\n",
    "2. Submit the link to your GitHub repository\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Wait for binning approval** from Abishek\n",
    "2. Move on to **Notebook 03: Classification Model**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}